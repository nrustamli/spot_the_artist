{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé® Spot the Artist - Anna Laurini Art  pupupu\n",
        "\n",
        "This notebook runs the AI backend for verifying Anna Laurini's street art using your **A100 GPU**.\n",
        "\n",
        "## Setup Instructions\n",
        "1. Go to **Runtime ‚Üí Change runtime type ‚Üí A100 GPU**\n",
        "2. Run all cells in order\n",
        "3. Upload your reference images when prompted\n",
        "4. Copy the ngrok URL to use with your frontend\n",
        "\n",
        "## üöÄ A100 Performance\n",
        "With your A100 GPU, you'll get:\n",
        "- **~5x faster** model loading compared to T4\n",
        "- **~10x faster** inference per image\n",
        "- Verification in **under 50ms** per image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU & Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"üöÄ GPU: {gpu_name}\")\n",
        "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "    if \"A100\" in gpu_name:\n",
        "        print(\"üî• A100 detected - Maximum performance!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí A100 GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q fastapi uvicorn python-multipart transformers Pillow pyngrok nest-asyncio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Reference Images\n",
        "\n",
        "Upload 15-20 images of Anna Laurini's artwork. These will be used to verify user uploads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create reference art folder\n",
        "os.makedirs('reference_art', exist_ok=True)\n",
        "\n",
        "print(\"üì§ Upload your reference images (Anna Laurini's artwork)\")\n",
        "print(\"   Supported formats: .jpg, .jpeg, .png, .webp\")\n",
        "print(\"   Recommended: 30-50 high-quality images for best accuracy\\n\")\n",
        "print(\"üí° TIP: You can select multiple files at once in the file picker!\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to reference_art folder\n",
        "for filename in uploaded.keys():\n",
        "    # Handle case where file might already exist\n",
        "    dest = f'reference_art/{filename}'\n",
        "    if os.path.exists(dest):\n",
        "        base, ext = os.path.splitext(filename)\n",
        "        dest = f'reference_art/{base}_copy{ext}'\n",
        "    os.rename(filename, dest)\n",
        "    print(f\"‚úÖ Saved: {os.path.basename(dest)}\")\n",
        "\n",
        "total = len([f for f in os.listdir('reference_art') if not f.startswith('.')])\n",
        "print(f\"\\nüìÅ Total reference images: {total}\")\n",
        "if total >= 30:\n",
        "    print(\"üéØ Great! 30+ images gives excellent accuracy.\")\n",
        "elif total >= 15:\n",
        "    print(\"üëç Good baseline. Consider adding more for better accuracy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create the CLIP Service\n",
        "\n",
        "This is the AI brain that compares images using OpenAI's CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "\n",
        "class CLIPService:\n",
        "    \"\"\"Service for verifying artwork using CLIP embeddings.\"\"\"\n",
        "    \n",
        "    MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "    SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n",
        "    \n",
        "    def __init__(self, reference_dir: str = \"reference_art\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"üîß Using device: {self.device}\")\n",
        "        \n",
        "        # Load CLIP model\n",
        "        print(f\"üì• Loading CLIP model: {self.MODEL_NAME}\")\n",
        "        self.model = CLIPModel.from_pretrained(self.MODEL_NAME).to(self.device)\n",
        "        self.processor = CLIPProcessor.from_pretrained(self.MODEL_NAME)\n",
        "        self.model.eval()\n",
        "        print(\"‚úÖ CLIP model loaded successfully\")\n",
        "        \n",
        "        # Store reference embeddings\n",
        "        self.reference_embeddings: Optional[torch.Tensor] = None\n",
        "        self.reference_names: list[str] = []\n",
        "        \n",
        "        # Load reference images\n",
        "        self.reference_dir = Path(reference_dir)\n",
        "        self._load_reference_images()\n",
        "    \n",
        "    def _load_reference_images(self) -> None:\n",
        "        \"\"\"Load and cache embeddings for all reference images.\"\"\"\n",
        "        if not self.reference_dir.exists():\n",
        "            print(f\"‚ö†Ô∏è Reference directory not found: {self.reference_dir}\")\n",
        "            return\n",
        "        \n",
        "        image_files = [\n",
        "            f for f in self.reference_dir.iterdir()\n",
        "            if f.suffix.lower() in self.SUPPORTED_EXTENSIONS\n",
        "        ]\n",
        "        \n",
        "        if not image_files:\n",
        "            print(f\"‚ö†Ô∏è No reference images found in {self.reference_dir}\")\n",
        "            return\n",
        "        \n",
        "        print(f\"üìö Loading {len(image_files)} reference images...\")\n",
        "        \n",
        "        embeddings = []\n",
        "        for img_path in image_files:\n",
        "            try:\n",
        "                embedding = self._get_image_embedding(img_path)\n",
        "                embeddings.append(embedding)\n",
        "                self.reference_names.append(img_path.name)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading {img_path.name}: {e}\")\n",
        "        \n",
        "        if embeddings:\n",
        "            self.reference_embeddings = torch.cat(embeddings, dim=0)\n",
        "            print(f\"‚úÖ Loaded {len(embeddings)} reference embeddings\")\n",
        "    \n",
        "    def _get_image_embedding(self, image_path: Path) -> torch.Tensor:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return self._embed_image(image)\n",
        "    \n",
        "    def _embed_image(self, image: Image.Image) -> torch.Tensor:\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "        with torch.no_grad():\n",
        "            image_features = self.model.get_image_features(**inputs)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features\n",
        "    \n",
        "    def verify_image(self, image: Image.Image) -> dict:\n",
        "        \"\"\"Verify if an image matches Anna Laurini's artwork.\n",
        "        Uses top-k matching for more robust verification.\"\"\"\n",
        "        if self.reference_embeddings is None or len(self.reference_embeddings) == 0:\n",
        "            return {\n",
        "                \"is_verified\": False,\n",
        "                \"confidence\": 0.0,\n",
        "                \"message\": \"No reference images loaded.\",\n",
        "                \"best_match\": None\n",
        "            }\n",
        "        \n",
        "        query_embedding = self._embed_image(image)\n",
        "        similarities = torch.mm(query_embedding, self.reference_embeddings.t()).squeeze(0)\n",
        "        \n",
        "        # Best match\n",
        "        best_similarity, best_idx = similarities.max(dim=0)\n",
        "        best_similarity = best_similarity.item()\n",
        "        best_match = self.reference_names[best_idx.item()]\n",
        "        \n",
        "        # Use top-k average for robust scoring\n",
        "        # Scale k based on number of references: ~10%, min 3, max 10\n",
        "        k = max(3, min(10, len(self.reference_names) // 10 + 1))\n",
        "        top_k_similarities, _ = similarities.topk(k)\n",
        "        avg_top_k = top_k_similarities.mean().item()\n",
        "        \n",
        "        confidence = self._scale_similarity(avg_top_k)\n",
        "        \n",
        "        # Stricter thresholds to reduce false positives\n",
        "        if confidence >= 80:\n",
        "            is_verified = True\n",
        "            message = \"‚úÖ Verified! This looks like Anna Laurini's artwork!\"\n",
        "        elif confidence >= 60:\n",
        "            is_verified = False\n",
        "            message = \"ü§î Uncertain. This might be Anna Laurini's art.\"\n",
        "        else:\n",
        "            is_verified = False\n",
        "            message = \"‚ùå Not recognized as Anna Laurini's artwork.\"\n",
        "        \n",
        "        return {\n",
        "            \"is_verified\": is_verified,\n",
        "            \"confidence\": round(confidence, 1),\n",
        "            \"message\": message,\n",
        "            \"best_match\": best_match,\n",
        "            \"raw_similarity\": round(best_similarity, 4),\n",
        "            \"avg_top_k\": round(avg_top_k, 4)\n",
        "        }\n",
        "    \n",
        "    def _scale_similarity(self, similarity: float) -> float:\n",
        "        \"\"\"Scale similarity to 0-100% with proper capping.\"\"\"\n",
        "        similarity = max(0.0, min(1.0, similarity))\n",
        "        \n",
        "        if similarity >= 0.80:\n",
        "            score = 90 + (similarity - 0.80) / 0.20 * 10\n",
        "            return min(100.0, score)  # Cap at 100%\n",
        "        elif similarity >= 0.70:\n",
        "            return 75 + (similarity - 0.70) / 0.10 * 15\n",
        "        elif similarity >= 0.55:\n",
        "            return 50 + (similarity - 0.55) / 0.15 * 25\n",
        "        elif similarity >= 0.40:\n",
        "            return 25 + (similarity - 0.40) / 0.15 * 25\n",
        "        else:\n",
        "            return similarity / 0.40 * 25\n",
        "    \n",
        "    def get_reference_count(self) -> int:\n",
        "        return len(self.reference_names)\n",
        "\n",
        "\n",
        "# Initialize the service\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "clip_service = CLIPService(\"reference_art\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test the Model (Optional)\n",
        "\n",
        "Upload a test image to verify everything works before starting the server.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Test with an image\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "print(\"üì§ Upload a test image to verify:\")\n",
        "test_upload = files.upload()\n",
        "\n",
        "for filename, data in test_upload.items():\n",
        "    image = Image.open(io.BytesIO(data)).convert(\"RGB\")\n",
        "    result = clip_service.verify_image(image)\n",
        "    \n",
        "    print(f\"\\nüñºÔ∏è Testing: {filename}\")\n",
        "    print(f\"   Verified: {result['is_verified']}\")\n",
        "    print(f\"   Confidence: {result['confidence']}%\")\n",
        "    print(f\"   Best match: {result['best_match']}\")\n",
        "    print(f\"   Message: {result['message']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create the FastAPI Server\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from PIL import Image\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"Anna Laurini Art Verification API\",\n",
        "    description=\"AI-powered verification of Anna Laurini street art\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Enable CORS for frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "class VerificationResponse(BaseModel):\n",
        "    is_verified: bool\n",
        "    confidence: float\n",
        "    message: str\n",
        "    best_match: str = None\n",
        "\n",
        "\n",
        "@app.get(\"/api/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"reference_images_loaded\": clip_service.get_reference_count(),\n",
        "        \"device\": clip_service.device\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/api/verify\", response_model=VerificationResponse)\n",
        "async def verify_artwork(file: UploadFile = File(...)):\n",
        "    \"\"\"Verify if an uploaded image matches Anna Laurini's artwork.\"\"\"\n",
        "    \n",
        "    if not file.content_type or not file.content_type.startswith(\"image/\"):\n",
        "        raise HTTPException(status_code=400, detail=\"Please upload an image file.\")\n",
        "    \n",
        "    try:\n",
        "        contents = await file.read()\n",
        "        image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n",
        "        result = clip_service.verify_image(image)\n",
        "        \n",
        "        return VerificationResponse(\n",
        "            is_verified=result[\"is_verified\"],\n",
        "            confidence=result[\"confidence\"],\n",
        "            message=result[\"message\"],\n",
        "            best_match=result.get(\"best_match\")\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing image: {str(e)}\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ FastAPI app created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Start the Server with ngrok\n",
        "\n",
        "### Setup (one-time):\n",
        "1. Go to [ngrok.com](https://ngrok.com) and create a free account\n",
        "2. Go to [dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken) ‚Üí Copy your **authtoken**\n",
        "3. Go to [dashboard.ngrok.com/domains](https://dashboard.ngrok.com/domains) ‚Üí Click **\"Create Domain\"** ‚Üí Copy your **domain** (e.g., `your-name-abc123.ngrok-free.app`)\n",
        "4. Paste both values in the cell below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This cell intentionally left empty - placeholder for ngrok token cell above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter your ngrok credentials here\n",
        "NGROK_AUTH_TOKEN = \"\"  # <-- Paste your authtoken here!\n",
        "NGROK_DOMAIN = \"\"      # <-- Paste your domain here (e.g., \"your-name-abc123.ngrok-free.app\")\n",
        "\n",
        "if not NGROK_AUTH_TOKEN:\n",
        "    print(\"‚ö†Ô∏è Missing authtoken!\")\n",
        "    print(\"   Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
        "elif not NGROK_DOMAIN:\n",
        "    print(\"‚ö†Ô∏è Missing domain!\")\n",
        "    print(\"   1. Go to: https://dashboard.ngrok.com/domains\")\n",
        "    print(\"   2. Click 'Create Domain' (it's free)\")\n",
        "    print(\"   3. Copy the domain (e.g., your-name-abc123.ngrok-free.app)\")\n",
        "else:\n",
        "    print(\"‚úÖ ngrok configured!\")\n",
        "    print(f\"   Token: {NGROK_AUTH_TOKEN[:10]}...\")\n",
        "    print(f\"   Domain: {NGROK_DOMAIN}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Apply nest_asyncio to allow running uvicorn in Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Configure ngrok with your authtoken\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel with your static domain\n",
        "# This is required for ngrok free tier (as of 2024)\n",
        "public_url = ngrok.connect(8000, domain=NGROK_DOMAIN)\n",
        "public_url_str = f\"https://{NGROK_DOMAIN}\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ SERVER IS RUNNING!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüåê Public URL: {public_url_str}\")\n",
        "print(f\"\\nüìã API Endpoints:\")\n",
        "print(f\"   ‚Ä¢ Health check: {public_url_str}/api/health\")\n",
        "print(f\"   ‚Ä¢ Verify art:   {public_url_str}/api/verify\")\n",
        "print(f\"   ‚Ä¢ API docs:     {public_url_str}/docs\")\n",
        "print(f\"\\nüí° To use with the React frontend:\")\n",
        "print(f\"   Set VITE_API_URL={public_url_str}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚ö†Ô∏è Keep this cell running! The server stops when you stop it.\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Run the server (this blocks until interrupted)\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Done!\n",
        "\n",
        "Your API is now running. You can:\n",
        "\n",
        "1. **Test the API** - Open the `/docs` URL in your browser to use the Swagger UI\n",
        "2. **Connect your frontend** - Update your React app to use the ngrok URL\n",
        "\n",
        "### To connect the React frontend:\n",
        "\n",
        "Create a `.env` file in the `frontend/` folder:\n",
        "```\n",
        "VITE_API_URL=https://xxxx-xx-xx-xx-xx.ngrok-free.app\n",
        "```\n",
        "\n",
        "Then run:\n",
        "```bash\n",
        "cd frontend\n",
        "npm run dev\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " "
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
